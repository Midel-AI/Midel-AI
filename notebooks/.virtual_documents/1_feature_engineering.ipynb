


from pathlib import Path
import dill
import matplotlib.pyplot as plt
import missingno as msno
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import seaborn as sns
from imblearn.pipeline import Pipeline as imb_Pipeline
from loguru import logger
from sklearn import set_config
from sklearn.compose import make_column_transformer, ColumnTransformer
from sklearn.dummy import DummyClassifier
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (confusion_matrix,
                             classification_report,
                             ConfusionMatrixDisplay,
                             roc_auc_score,
                             accuracy_score,
                             precision_score,
                             recall_score,
                             f1_score,
                             RocCurveDisplay,
                             PrecisionRecallDisplay,
                            )
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OrdinalEncoder, OneHotEncoder
from ucimlrepo import fetch_ucirepo, list_available_datasets
from yellowbrick.classifier import DiscriminationThreshold


HOME_DIR = Path("C:/Users/HP/Desktop/Livrables_groupe6/DATA")
DATA_TRAIN = HOME_DIR / "training.csv"
df = pd.read_csv(DATA_TRAIN, sep=";")





# Conversion de la variable 'PricingStrategy' en type string (catégorielle)
# Cette variable représente une stratégie tarifaire appliquée par Xente. 
# Même si elle est codée sous forme de chiffres (ex : 0, 1, 2...), elle ne représente pas une quantité ou un ordre logique,
# mais plutôt des catégories distinctes. Il est donc plus pertinent de la traiter comme une variable catégorielle.
df['PricingStrategy'] = df['PricingStrategy'].astype('str')



categorical_columns = df.select_dtypes(include="object").columns
categorical_columns



# --- Création de variables dérivées à partir des colonnes 'AccountId', 'SubscriptionId' et 'CustomerId' ---

# 1. Somme des montants absolus par client
# Cela permet de capturer le volume total de transactions (débits + crédits) réalisé par chaque client.
# Un comportement anormalement élevé peut être un indicateur de fraude.
df['CustomerId_abs_amount_sum'] = df.groupby('CustomerId')['Amount'].transform(lambda x: x.abs().sum())

# 2. Nombre total de transactions par abonnement
# Une activité trop fréquente sur une même souscription peut indiquer une anomalie.
df['SubscriptionId_transaction_count'] = df.groupby('SubscriptionId')['TransactionId'].transform('count')

# 3. Écart-type des montants absolus par client
# Cette statistique mesure la variation des montants dépensés par un client.
# Une grande variabilité peut indiquer un comportement irrégulier ou suspect.
df['CustomerId_abs_amount_std'] = df.groupby('CustomerId')['Amount'].transform(lambda x: x.abs().std())

# 4. Remplacement des valeurs manquantes dans l'écart-type par 0
# Certaines séries peuvent contenir une seule transaction, ce qui rend impossible le calcul de l’écart-type.
# Dans ce cas, on considère qu’il n’y a pas de variabilité.
df['CustomerId_abs_amount_std'] = df['CustomerId_abs_amount_std'].fillna(0)



# --- Création de variables dérivées à partir de la colonne temporelle 'TransactionStartTime' ---

# 1. Création d'une variable catégorielle pour le type de transaction : crédit ou débit
# Cela permet de différencier les flux sortants (débits) des flux entrants (crédits),
# ce qui peut s’avérer utile pour détecter certains comportements suspects.
df['TransactionType'] = df['Amount'].apply(lambda x: 'Credit' if x < 0 else 'Debit').astype('object')

# 2. Conversion de la variable temporelle au format datetime
# Nécessaire pour pouvoir extraire correctement des composantes temporelles.
df['TransactionStartTime'] = pd.to_datetime(df['TransactionStartTime'])

# 3. Extraction de l’heure et du jour de la semaine à partir de la date de transaction
# L’heure (TransactionHour) peut révéler des comportements atypiques (ex : transactions la nuit).
# Le jour (TransactionDay) peut également être pertinent si certaines fraudes surviennent plus souvent certains jours.
df['TransactionHour'] = df['TransactionStartTime'].dt.hour
df['TransactionDay'] = df['TransactionStartTime'].dt.day_name(locale='fr_FR')

# 4. Création d'une variable 'MomentOfDay' pour catégoriser l'heure en périodes de la journée
# Cela permet de simplifier la variable horaire en trois grandes plages horaires :
# matin (6h-13h), après-midi (13h-20h), et nuit (20h-6h),
# ce qui peut rendre l’information plus facilement exploitable dans un modèle.
def get_moment(hour):
    if 6 <= hour < 13:
        return 'matin'
    elif 13 <= hour <= 20:
        return 'apres-midi'
    else:
        return 'nuit'

df['MomentOfDay'] = df['TransactionHour'].apply(get_moment)



# --- Vérification avant suppression de 'ProductCategory' ---

# Avant de supprimer la variable 'ProductCategory', on s'assure qu'elle est redondante avec 'ProductId'.
# Concrètement, on veut vérifier qu'à chaque 'ProductId' correspond une seule et unique 'ProductCategory'.
# Si cette condition est remplie, alors 'ProductCategory' n’apporte pas d’information supplémentaire
# et peut être supprimée sans perte d’information (car elle est entièrement déterminée par 'ProductId').

mapping_check = df.groupby("ProductId")["ProductCategory"].nunique()
print("Nombre de ProductId ayant plus d'une catégorie :", (mapping_check > 1).sum())



# Ces colonnes n’apportent plus d'information utile au modèle
cols_to_drop = ['TransactionId', 'BatchId', 'CustomerId','AccountId', 'SubscriptionId', 
                'CountryCode', 'CurrencyCode','TransactionStartTime','TransactionHour','ProductCategory']
df.drop(columns=cols_to_drop, inplace=True)


# les variables categorielles obtenues apres traitement 
categorical_columns = df.select_dtypes(include="object").columns
categorical_columns





# Recoder la variable ProduitId 
# Calculer la fréquence des produits
frequencies_product = df['ProductId'].value_counts(normalize=True) * 100

# Sélectionner les produits dont la fréquence dépasse 12%
products_above_12 = frequencies_product[frequencies_product > 12]
products_list = products_above_12.index.tolist()
def recode_product(X):
  try:
        if 'ProductId' in X.columns:
            X = X.copy()
            X['ProductId'] = X['ProductId'].apply(lambda x: x if x in products_list else 'Other')
        return X
  except:
        print("Vérifier la liste des colonnes")

df = recode_product(df)



# Recoder la variable providerID 
frequencies_provider = df['ProviderId'].value_counts(normalize=True) * 100

# Sélectionner les provider dont la fréquence dépasse 12%
provider_above_12 = frequencies_provider[frequencies_provider > 12]

provider_list = provider_above_12.index.tolist()

def recode_provider(X):
  try:
        if 'ProviderId' in X.columns:
            X = X.copy()
            X['ProviderId'] = X['ProviderId'].apply(lambda x: x if x in provider_list else 'Other')
        return X
  except:
        print("Vérifier la liste des colonnes")

df = recode_provider(df)



# Recoder la variable ChannelId

channel_list = ['ChannelId_2','ChannelId_3']
def recode_channel(X):
  try:
        if 'ChannelId' in X.columns:
            X = X.copy()
            X['ChannelId'] = X['ChannelId'].apply(lambda x: x if x in channel_list else 'Other')
        return X
  except:
        print("Vérifier la liste des colonnes")

df = recode_channel(df)



# Recoder la variable PricingStrategy
Pricing_list = ["2","4"]

def recode_pricing(X):
  try:
        if 'PricingStrategy' in X.columns:
            X = X.copy()
            X['PricingStrategy'] = X['PricingStrategy'].apply(lambda x: x if x in Pricing_list else 'Other')
        return X
  except:
        print("Vérifier la liste des colonnes")

df = recode_pricing(df)






numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
numeric_columns = [col for col in numeric_columns if col != 'FraudResult']
numeric_columns


# Transformation logarithmique de la somme absolue des montants par client.
# Cette variable présente une meilleure distribution que la variable d'origine (selon le F-test),
# ce qui améliore la significativité dans les modèles et atténue l'effet des valeurs extrêmes.
df['log_CustomerId_abs_amount_sum'] = np.log1p(df['CustomerId_abs_amount_sum'])

# Création d'une variable capturant l’écart entre la valeur estimée d’une transaction (Value)
# et le montant réellement débité/crédité (Amount). Un écart inhabituel peut indiquer une incohérence,
# potentiellement liée à une fraude
df['Amount_Value_Ecart'] = df['Value'] - df['Amount'].abs()



numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
numeric_columns = [col for col in numeric_columns if col != 'FraudResult']
numeric_columns



# Suppression de certaines variables devenues redondantes ou moins informatives.
# La variable 'Amount' est fortement corrélée à 'Value', or cette dernière s'est révélée plus significative selon les tests statistiques
# Quant à 'CustomerId_abs_amount_sum', sa version transformée en logarithme 
# ('log_CustomerId_abs_amount_sum') est plus discriminante pour capturer les comportements frauduleux selon les tests statistiques.
cols_to_drop = ['Amount', 'CustomerId_abs_amount_sum']
df.drop(columns=cols_to_drop, inplace=True)







