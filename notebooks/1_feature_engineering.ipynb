{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f53326-5665-4305-9690-61b1f2ea18d5",
   "metadata": {},
   "source": [
    "#  Objectif du notebook : Feature Engineering\n",
    "L'objectif de cette étape est de créer des variables dérivées à partir des colonnes existantes afin d’enrichir l'information disponible pour les modèles de machine learning. Ce processus vise à renforcer la capacité prédictive des modèles en capturant des motifs ou relations non directement visibles dans les données brutes.\n",
    "Nous procéderons également à la suppression de certaines variables jugées peu informatives, redondantes ou non pertinentes pour la détection de la fraude. Cette phase est donc essentielle pour améliorer la performance, la robustesse et la lisibilité du modèle final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affe341a-f277-4e1d-b4f9-b1a0414c1043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import dill\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from imblearn.pipeline import Pipeline as imb_Pipeline\n",
    "from loguru import logger\n",
    "from sklearn import set_config\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (confusion_matrix,\n",
    "                             classification_report,\n",
    "                             ConfusionMatrixDisplay,\n",
    "                             roc_auc_score,\n",
    "                             accuracy_score,\n",
    "                             precision_score,\n",
    "                             recall_score,\n",
    "                             f1_score,\n",
    "                             RocCurveDisplay,\n",
    "                             PrecisionRecallDisplay,\n",
    "                            )\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OrdinalEncoder, OneHotEncoder\n",
    "from ucimlrepo import fetch_ucirepo, list_available_datasets\n",
    "from yellowbrick.classifier import DiscriminationThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1c300b5-3e17-4abf-ba57-8df3fb3d109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = Path(\"data\")\n",
    "DATA_TRAIN = HOME_DIR / \"training.csv\"\n",
    "HOME_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfec9dc9-9806-4838-8bc5-ff1dc92144d7",
   "metadata": {},
   "source": [
    "# Traitement des variables catégorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc7ebed7-f715-46b2-9898-a6a8559415b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de la variable 'PricingStrategy' en type string (catégorielle)\n",
    "# Cette variable représente une stratégie tarifaire appliquée par Xente. \n",
    "# Même si elle est codée sous forme de chiffres (ex : 0, 1, 2...), elle ne représente pas une quantité ou un ordre logique,\n",
    "# mais plutôt des catégories distinctes. Il est donc plus pertinent de la traiter comme une variable catégorielle.\n",
    "df['PricingStrategy'] = df['PricingStrategy'].astype('str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b90011f-fb20-41cb-9719-270d092122d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId',\n",
       "       'CurrencyCode', 'ProviderId', 'ProductId', 'ProductCategory',\n",
       "       'ChannelId', 'TransactionStartTime', 'PricingStrategy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = df.select_dtypes(include=\"object\").columns\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb2154ec-63d4-4eab-a60f-248164816280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Création de variables dérivées à partir des colonnes 'AccountId', 'SubscriptionId' et 'CustomerId' ---\n",
    "\n",
    "# 1. Somme des montants absolus par client\n",
    "# Cela permet de capturer le volume total de transactions (débits + crédits) réalisé par chaque client.\n",
    "# Un comportement anormalement élevé peut être un indicateur de fraude.\n",
    "df['CustomerId_abs_amount_sum'] = df.groupby('CustomerId')['Amount'].transform(lambda x: x.abs().sum())\n",
    "\n",
    "# 2. Nombre total de transactions par abonnement\n",
    "# Une activité trop fréquente sur une même souscription peut indiquer une anomalie.\n",
    "df['SubscriptionId_transaction_count'] = df.groupby('SubscriptionId')['TransactionId'].transform('count')\n",
    "\n",
    "# 3. Écart-type des montants absolus par client\n",
    "# Cette statistique mesure la variation des montants dépensés par un client.\n",
    "# Une grande variabilité peut indiquer un comportement irrégulier ou suspect.\n",
    "df['CustomerId_abs_amount_std'] = df.groupby('CustomerId')['Amount'].transform(lambda x: x.abs().std())\n",
    "\n",
    "# 4. Remplacement des valeurs manquantes dans l'écart-type par 0\n",
    "# Certaines séries peuvent contenir une seule transaction, ce qui rend impossible le calcul de l’écart-type.\n",
    "# Dans ce cas, on considère qu’il n’y a pas de variabilité.\n",
    "df['CustomerId_abs_amount_std'] = df['CustomerId_abs_amount_std'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "626023c2-2e52-4c91-8b77-a0dacff308c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Création de variables dérivées à partir de la colonne temporelle 'TransactionStartTime' ---\n",
    "\n",
    "# 1. Création d'une variable catégorielle pour le type de transaction : crédit ou débit\n",
    "# Cela permet de différencier les flux sortants (débits) des flux entrants (crédits),\n",
    "# ce qui peut s’avérer utile pour détecter certains comportements suspects.\n",
    "df['TransactionType'] = df['Amount'].apply(lambda x: 'Credit' if x < 0 else 'Debit').astype('object')\n",
    "\n",
    "# 2. Conversion de la variable temporelle au format datetime\n",
    "# Nécessaire pour pouvoir extraire correctement des composantes temporelles.\n",
    "df['TransactionStartTime'] = pd.to_datetime(df['TransactionStartTime'])\n",
    "\n",
    "# 3. Extraction de l’heure et du jour de la semaine à partir de la date de transaction\n",
    "# L’heure (TransactionHour) peut révéler des comportements atypiques (ex : transactions la nuit).\n",
    "# Le jour (TransactionDay) peut également être pertinent si certaines fraudes surviennent plus souvent certains jours.\n",
    "df['TransactionHour'] = df['TransactionStartTime'].dt.hour\n",
    "df['TransactionDay'] = df['TransactionStartTime'].dt.day_name(locale='fr_FR')\n",
    "\n",
    "# 4. Création d'une variable 'MomentOfDay' pour catégoriser l'heure en périodes de la journée\n",
    "# Cela permet de simplifier la variable horaire en trois grandes plages horaires :\n",
    "# matin (6h-13h), après-midi (13h-20h), et nuit (20h-6h),\n",
    "# ce qui peut rendre l’information plus facilement exploitable dans un modèle.\n",
    "def get_moment(hour):\n",
    "    if 6 <= hour < 13:\n",
    "        return 'matin'\n",
    "    elif 13 <= hour <= 20:\n",
    "        return 'apres-midi'\n",
    "    else:\n",
    "        return 'nuit'\n",
    "\n",
    "df['MomentOfDay'] = df['TransactionHour'].apply(get_moment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53046a62-750c-4bc5-92bd-da53380121ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de ProductId ayant plus d'une catégorie : 0\n"
     ]
    }
   ],
   "source": [
    "# --- Vérification avant suppression de 'ProductCategory' ---\n",
    "\n",
    "# Avant de supprimer la variable 'ProductCategory', on s'assure qu'elle est redondante avec 'ProductId'.\n",
    "# Concrètement, on veut vérifier qu'à chaque 'ProductId' correspond une seule et unique 'ProductCategory'.\n",
    "# Si cette condition est remplie, alors 'ProductCategory' n’apporte pas d’information supplémentaire\n",
    "# et peut être supprimée sans perte d’information (car elle est entièrement déterminée par 'ProductId').\n",
    "\n",
    "mapping_check = df.groupby(\"ProductId\")[\"ProductCategory\"].nunique()\n",
    "print(\"Nombre de ProductId ayant plus d'une catégorie :\", (mapping_check > 1).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c79e3e4b-88ff-4c7e-abfe-1b899c2d0018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ces colonnes n’apportent plus d'information utile au modèle\n",
    "cols_to_drop = ['TransactionId', 'BatchId', 'CustomerId','AccountId', 'SubscriptionId', \n",
    "                'CountryCode', 'CurrencyCode','TransactionStartTime','TransactionHour','ProductCategory']\n",
    "df.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9badd9d0-ba03-4b99-8517-c23c1ec26c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ProviderId', 'ProductId', 'ChannelId', 'PricingStrategy',\n",
       "       'TransactionType', 'TransactionDay', 'MomentOfDay'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# les variables categorielles obtenues apres traitement \n",
    "categorical_columns = df.select_dtypes(include=\"object\").columns\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0325fe-8bf0-462f-8a31-64ca4da3da03",
   "metadata": {},
   "source": [
    "## Recodage de certaines variables catégorielles \n",
    "Nous avons procédé à un regroupement des modalités des variables ProviderId, ProductId, ChannelId et PricingStrategy dans le but de réduire leur cardinalité. Cette étape est essentielle en prévision d’un encodage one-hot, afin d’éviter une explosion du nombre de variables qui pourrait nuire à la performance et à l’interprétabilité du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18cae24c-650f-4262-91bc-3f26ccbcd709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recoder la variable ProduitId \n",
    "# Calculer la fréquence des produits\n",
    "frequencies_product = df['ProductId'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Sélectionner les produits dont la fréquence dépasse 12%\n",
    "products_above_12 = frequencies_product[frequencies_product > 12]\n",
    "products_list = products_above_12.index.tolist()\n",
    "def recode_product(X):\n",
    "  try:\n",
    "        if 'ProductId' in X.columns:\n",
    "            X = X.copy()\n",
    "            X['ProductId'] = X['ProductId'].apply(lambda x: x if x in products_list else 'Other')\n",
    "        return X\n",
    "  except:\n",
    "        print(\"Vérifier la liste des colonnes\")\n",
    "\n",
    "df = recode_product(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d22bf5b4-7cca-438f-b9d7-e05987933921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recoder la variable providerID \n",
    "frequencies_provider = df['ProviderId'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Sélectionner les provider dont la fréquence dépasse 12%\n",
    "provider_above_12 = frequencies_provider[frequencies_provider > 12]\n",
    "\n",
    "provider_list = provider_above_12.index.tolist()\n",
    "\n",
    "def recode_provider(X):\n",
    "  try:\n",
    "        if 'ProviderId' in X.columns:\n",
    "            X = X.copy()\n",
    "            X['ProviderId'] = X['ProviderId'].apply(lambda x: x if x in provider_list else 'Other')\n",
    "        return X\n",
    "  except:\n",
    "        print(\"Vérifier la liste des colonnes\")\n",
    "\n",
    "df = recode_provider(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e52e5009-cb9d-4b4a-bb17-19ce1dfcac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recoder la variable ChannelId\n",
    "\n",
    "channel_list = ['ChannelId_2','ChannelId_3']\n",
    "def recode_channel(X):\n",
    "  try:\n",
    "        if 'ChannelId' in X.columns:\n",
    "            X = X.copy()\n",
    "            X['ChannelId'] = X['ChannelId'].apply(lambda x: x if x in channel_list else 'Other')\n",
    "        return X\n",
    "  except:\n",
    "        print(\"Vérifier la liste des colonnes\")\n",
    "\n",
    "df = recode_channel(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70f9f9b5-0516-40f4-add4-f9e658a1218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recoder la variable PricingStrategy\n",
    "Pricing_list = [\"2\",\"4\"]\n",
    "\n",
    "def recode_pricing(X):\n",
    "  try:\n",
    "        if 'PricingStrategy' in X.columns:\n",
    "            X = X.copy()\n",
    "            X['PricingStrategy'] = X['PricingStrategy'].apply(lambda x: x if x in Pricing_list else 'Other')\n",
    "        return X\n",
    "  except:\n",
    "        print(\"Vérifier la liste des colonnes\")\n",
    "\n",
    "df = recode_pricing(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2246a0-72b1-4afd-9d8a-6d17275c86a5",
   "metadata": {},
   "source": [
    "# Traitement des variables numériques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75badade-8677-4ff1-983e-eb5a228c4f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amount',\n",
       " 'Value',\n",
       " 'CustomerId_abs_amount_sum',\n",
       " 'SubscriptionId_transaction_count',\n",
       " 'CustomerId_abs_amount_std']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "numeric_columns = [col for col in numeric_columns if col != 'FraudResult']\n",
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "428463a1-f948-43aa-ade9-12b6c41c2d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation logarithmique de la somme absolue des montants par client.\n",
    "# Cette variable présente une meilleure distribution que la variable d'origine (selon le F-test),\n",
    "# ce qui améliore la significativité dans les modèles et atténue l'effet des valeurs extrêmes.\n",
    "df['log_CustomerId_abs_amount_sum'] = np.log1p(df['CustomerId_abs_amount_sum'])\n",
    "\n",
    "# Création d'une variable capturant l’écart entre la valeur estimée d’une transaction (Value)\n",
    "# et le montant réellement débité/crédité (Amount). Un écart inhabituel peut indiquer une incohérence,\n",
    "# potentiellement liée à une fraude\n",
    "df['Amount_Value_Ecart'] = df['Value'] - df['Amount'].abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f05352f-601e-4b53-aa43-abcf39f9aebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Value',\n",
       " 'SubscriptionId_transaction_count',\n",
       " 'CustomerId_abs_amount_std',\n",
       " 'log_CustomerId_abs_amount_sum',\n",
       " 'Amount_Value_Ecart']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "numeric_columns = [col for col in numeric_columns if col != 'FraudResult']\n",
    "numeric_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d35ebe7-e54a-4748-b31d-8f1f917b67c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de certaines variables devenues redondantes ou moins informatives.\n",
    "# La variable 'Amount' est fortement corrélée à 'Value', or cette dernière s'est révélée plus significative selon les tests statistiques\n",
    "# Quant à 'CustomerId_abs_amount_sum', sa version transformée en logarithme \n",
    "# ('log_CustomerId_abs_amount_sum') est plus discriminante pour capturer les comportements frauduleux selon les tests statistiques.\n",
    "cols_to_drop = ['Amount', 'CustomerId_abs_amount_sum']\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b028a18-cae9-4678-bc89-c7a9851309d0",
   "metadata": {},
   "source": [
    "# Encodage des Variables Catégorielles\n",
    "Les variables catégorielles sont encodées à l'aide de la méthode One-Hot Encoding. Cette méthode est utilisée pour transformer les variables qualitatives en variables numériques binaires. Concrètement, chaque catégorie d'une variable est convertie en une colonne distincte avec des valeurs 0 ou 1, indiquant la présence ou l'absence de cette catégorie. Cette technique permet aux modèles de comprendre les relations entre les différentes catégories, en évitant de leur attribuer une notion d'ordre ou de distance qui pourrait fausser les résultats si elles étaient simplement codées numériquement. L'encodage One-Hot est essentiel pour garantir que le modèle n'interprète pas les catégories comme ayant une hiérarchie ou des valeurs numériques implicites.\n",
    "\n",
    "# Standardisation des Variables Numériques\n",
    "Les variables numériques sont standardisées. Cette opération consiste à transformer les variables pour qu'elles aient une moyenne de 0 et un écart-type de 1. Cela permet d'harmoniser les échelles des différentes variables numériques, ce qui est crucial pour certains algorithmes d'apprentissage automatique, tels que les régressions, les k-plus proches voisins (k-NN), ou les réseaux neuronaux, qui sont sensibles à l'échelle des données. En standardisant les données, nous nous assurons que toutes les variables contribuent de manière égale à la modélisation, ce qui permet d'éviter qu'une variable avec une grande échelle n'éclipse l'impact d'une autre variable.\n",
    "\n",
    "# Application dans les Pipelines des Modèles\n",
    "Ces transformations, à savoir l'encodage One-Hot des variables catégorielles et la standardisation des variables numériques, sont intégrées directement dans les pipelines des modèles. Cela permet d'assurer que chaque donnée passe par ces étapes de transformation avant d'être utilisée pour l'entraînement du modèle, ce qui garantit une gestion cohérente et reproductible des prétraitements des données.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511cd159-eeb1-4f05-82fd-263d74256bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
